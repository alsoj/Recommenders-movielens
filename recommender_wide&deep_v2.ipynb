{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Wide & Deep 튜토리얼 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Dataset 100K 용 세팅\n",
    "#######################\n",
    "import numpy as np\n",
    "\n",
    "# Load each data set (users, movies, and ratings).\n",
    "users_cols = ['userid', 'age', 'gender', 'occupation', 'zip_code']\n",
    "users = pd.read_csv('./data/100K/u.user', sep='|', names=users_cols, encoding='latin-1')\n",
    "\n",
    "ratings_cols = ['userid', 'movieid', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv('./data/100K/u.data', sep='\\t', names=ratings_cols, encoding='latin-1')\n",
    "\n",
    "# The movies file contains a binary feature for each genre.\n",
    "genre_cols = [\n",
    "    \"genre_unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n",
    "    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
    "    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
    "]\n",
    "movies_cols = [\n",
    "    ITEM_COL, 'title', 'release_date', \"video_release_date\", \"imdb_url\"\n",
    "] + genre_cols\n",
    "movies = pd.read_csv(\n",
    "    './data/100K/u.item', sep='|', names=movies_cols, encoding='latin-1')\n",
    "\n",
    "# Since the ids start at 1, we shift them to start at 0.\n",
    "users[USER_COL] = users[USER_COL].apply(lambda x: str(x-1))\n",
    "movies[ITEM_COL] = movies[ITEM_COL].apply(lambda x: str(x-1))\n",
    "movies[\"year\"] = movies['release_date'].apply(lambda x: str(x).split('-')[-1])\n",
    "ratings[ITEM_COL] = ratings[ITEM_COL].apply(lambda x: str(x-1))\n",
    "ratings[USER_COL] = ratings[USER_COL].apply(lambda x: str(x-1))\n",
    "ratings[RATING_COL] = ratings[RATING_COL].apply(lambda x: float(x))\n",
    "\n",
    "# Compute the number of movies to which a genre is assigned.\n",
    "genre_occurences = movies[genre_cols].sum().to_dict()\n",
    "\n",
    "# Since some movies can belong to more than one genre, we create different\n",
    "# 'genre' columns as follows:\n",
    "# - all_genres: all the active genres of the movie.\n",
    "# - genre: randomly sampled from the active genres.\n",
    "def mark_genres(movies, genres):\n",
    "  def get_random_genre(gs):\n",
    "    active = [genre for genre, g in zip(genres, gs) if g==1]\n",
    "    if len(active) == 0:\n",
    "      return 'Other'\n",
    "    return np.random.choice(active)\n",
    "  def get_all_genres(gs):\n",
    "    active = [genre for genre, g in zip(genres, gs) if g==1]\n",
    "    if len(active) == 0:\n",
    "      return 'Other'\n",
    "    return '|'.join(active)\n",
    "  movies['Genres'] = [\n",
    "      get_random_genre(gs) for gs in zip(*[movies[genre] for genre in genres])]\n",
    "  movies['Genres_string'] = [\n",
    "      get_all_genres(gs) for gs in zip(*[movies[genre] for genre in genres])]\n",
    "\n",
    "mark_genres(movies, genre_cols)\n",
    "\n",
    "# Create one merged DataFrame containing all the movielens data.\n",
    "movielens = ratings.merge(movies, on=ITEM_COL).merge(users, on=USER_COL)\n",
    "\n",
    "movielens = movielens.fillna(0)\n",
    "\n",
    "print(\"movielens::: \\n\", movielens.head())\n",
    "df_data = movielens\n",
    "\n",
    "\n",
    "###############################\n",
    "# 데이터 전처리\n",
    "# 2-1 Genre 인코딩\n",
    "###############################\n",
    "# Encode 'genres' into int array (multi-hot representation) to use as item features\n",
    "genres_encoder = sklearn.preprocessing.MultiLabelBinarizer()\n",
    "df_data['Genres'] = genres_encoder.fit_transform(\n",
    "    df_data['Genres_string'].apply(lambda s: s.split(\"|\"))\n",
    ").tolist()\n",
    "\n",
    "###############################\n",
    "# 데이터 전처리\n",
    "# 2-2 sex 인코딩\n",
    "###############################\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "df_data['sex'] = le.fit_transform(df_data['sex'])\n",
    "\n",
    "\n",
    "# 기존 트레인 데이터 : UserId  MovieId  Rating   timestamp  MovieName  Genres\n",
    "df_data['MovieName'] = df_data['title']\n",
    "\n",
    "df_data_new = df_data[['userid','movieid','rating','movie_name','genres', 'sex', 'age']]\n",
    "df_data = df_data_new\n",
    "print(\"df_data::: \\n\", df_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical base columns.\n",
    "gender = tf.contrib.layers.sparse_column_with_keys(column_name=\"gender\", keys=[\"female\", \"male\"])\n",
    "genre = tf.contrib.layers.sparse_column_with_keys(column_name=\"genre\", keys=[\n",
    "  \"genre_unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n",
    "    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
    "    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"])\n",
    "occupation = tf.contrib.layers.sparse_column_with_hash_bucket(\"occupation\", hash_bucket_size=1000)\n",
    "zip_code = tf.contrib.layers.sparse_column_with_hash_bucket(\"zip_code\", hash_bucket_size=100)\n",
    "\n",
    "# Continuous base columns.\n",
    "age = tf.contrib.layers.real_valued_column(\"age\")\n",
    "age_buckets = tf.contrib.layers.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "release_date = tf.contrib.layers.real_valued_column(\"release_date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_columns = [\n",
    "  gender, genre, occupation, zip_code, release_date, age_buckets,   \n",
    "  tf.contrib.layers.crossed_column([gender, genre], hash_bucket_size=int(1e4)),\n",
    "  tf.contrib.layers.crossed_column([age_buckets, genre], hash_bucket_size=int(1e4)),\n",
    "  tf.contrib.layers.crossed_column([occupation, genre], hash_bucket_size=int(1e6))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 20:17:45.447030 18472 feature_column.py:1091] The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 20:17:45.449023 18472 feature_column.py:1091] The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 20:17:45.451018 18472 feature_column.py:1091] The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 20:17:45.453011 18472 feature_column.py:1091] The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n"
     ]
    }
   ],
   "source": [
    "deep_columns = [\n",
    "  tf.contrib.layers.embedding_column(gender, dimension=8),\n",
    "  tf.contrib.layers.embedding_column(genre, dimension=8),\n",
    "  tf.contrib.layers.embedding_column(occupation, dimension=8),\n",
    "  tf.contrib.layers.embedding_column(zip_code, dimension=8),\n",
    "  age, release_date\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 20:18:00.824897 18472 deprecation.py:573] From <ipython-input-9-3507e962daab>:7: calling DNNLinearCombinedClassifier.__init__ (from tensorflow.contrib.learn.python.learn.estimators.dnn_linear_combined) with fix_global_step_increment_bug=False is deprecated and will be removed after 2017-04-15.\nInstructions for updating:\nPlease set fix_global_step_increment_bug=True and update training steps in your pipeline. See pydoc for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 20:18:00.826891 18472 deprecation.py:323] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\venv\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn_linear_combined.py:676: multi_class_head (from tensorflow.contrib.learn.python.learn.estimators.head) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease switch to tf.contrib.estimator.*_head.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 20:18:00.833874 18472 deprecation.py:323] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\venv\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py:1179: BaseEstimator.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease replace uses of any Estimator from tf.contrib.learn with an Estimator from tf.estimator.*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 20:18:00.835867 18472 deprecation.py:323] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\venv\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py:427: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\nInstructions for updating:\nWhen switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "model_dir = tempfile.mkdtemp()\n",
    "m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "    model_dir=model_dir,\n",
    "    linear_feature_columns=wide_columns,\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    dnn_hidden_units=[100, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "# Define the column names for the data sets.\n",
    "COLUMNS = [\"gender\", \"genre\", \"occupation\", \"zip_code\", \"age\", \"age_buckets\", \"release_date\"]\n",
    "LABEL_COLUMN = 'Rating'\n",
    "CATEGORICAL_COLUMNS = [\"gender\", \"genre\", \"occupation\", \"zip_code\"]\n",
    "CONTINUOUS_COLUMNS = [\"age\", \"release_date\"]\n",
    "\n",
    "\n",
    "# Read the training and test data sets into Pandas dataframe.\n",
    "df_data = \n",
    "df_train = pd.read_csv(train_file, names=COLUMNS, skipinitialspace=True)\n",
    "df_test = pd.read_csv(test_file, names=COLUMNS, skipinitialspace=True, skiprows=1)\n",
    "df_train[LABEL_COLUMN] = (df_train['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)\n",
    "df_test[LABEL_COLUMN] = (df_test['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)\n",
    "\n",
    "def input_fn(df):\n",
    "  # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "  # the values of that column stored in a constant Tensor.\n",
    "  continuous_cols = {k: tf.constant(df[k].values)\n",
    "                     for k in CONTINUOUS_COLUMNS}\n",
    "  # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "  # to the values of that column stored in a tf.SparseTensor.\n",
    "  categorical_cols = {k: tf.SparseTensor(\n",
    "      indices=[[i, 0] for i in range(df[k].size)],\n",
    "      values=df[k].values,\n",
    "      shape=[df[k].size, 1])\n",
    "                      for k in CATEGORICAL_COLUMNS}\n",
    "  # Merges the two dictionaries into one.\n",
    "  feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n",
    "  # Converts the label column into a constant Tensor.\n",
    "  label = tf.constant(df[LABEL_COLUMN].values)\n",
    "  # Returns the feature columns and the label.\n",
    "  return feature_cols, label\n",
    "\n",
    "def train_input_fn():\n",
    "  return input_fn(df_train)\n",
    "\n",
    "def eval_input_fn():\n",
    "  return input_fn(df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
