{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide & Deep Recommendation System with Movie Lens\n",
    "출처 : [Microsoft Github] (https://github.com/microsoft/recommenders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:31:29.863954 11028 deprecation_wrapper.py:119] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\tf_utils.py:167: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0']"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num CPUs:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "import papermill as pm\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "from python_splitters import python_random_split\n",
    "import wide_deep_utils as wide_deep\n",
    "import tf_utils\n",
    "from pandas_df_utils import user_item_pairs\n",
    "import python_evaluation\n",
    "\n",
    "print(\"Tensorflow Version:\", tf.VERSION)\n",
    "devices = device_lib.list_local_devices()\n",
    "print([x.name for x in devices])\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "print(\"Num CPUs:\", num_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# 파라미터 세팅\n",
    "####################\n",
    "#Recommend top k items\n",
    "TOP_K = 10\n",
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'\n",
    "# Metrics to use for evaluation. reco_utils.evaluation.python_evaluation function names\n",
    "RANKING_METRICS = ['map_at_k', 'ndcg_at_k', 'precision_at_k', 'recall_at_k']\n",
    "RATING_METRICS = ['rmse', 'mae', 'rsquared', 'exp_var']\n",
    "# Use session hook to evaluate model while training\n",
    "EVALUATE_WHILE_TRAINING = True\n",
    "\n",
    "# Data column names\n",
    "USER_COL = 'UserId'\n",
    "ITEM_COL = 'MovieId'\n",
    "RATING_COL = 'Rating'\n",
    "ITEM_FEAT_COL = 'Genres'\n",
    "\n",
    "# Train and test set pickle file paths. If None, download and split the dataset.\n",
    "DATA_DIR = None\n",
    "TRAIN_PICKLE_PATH = None\n",
    "TEST_PICKLE_PATH = None\n",
    "EXPORT_DIR_BASE = './outputs/model'\n",
    "\n",
    "#### Hyperparameters\n",
    "MODEL_TYPE = 'wide_deep'\n",
    "EPOCHS = 50  # if 0, only 1 batch will be processed\n",
    "BATCH_SIZE = 64\n",
    "# Wide (linear) model hyperparameters\n",
    "LINEAR_OPTIMIZER = 'Ftrl'\n",
    "LINEAR_OPTIMIZER_LR =0.0029   # Learning rate\n",
    "LINEAR_L1_REG = 0.0           # L1 Regularization rate for FtrlOptimizer\n",
    "LINEAR_MOMENTUM = 0.9         # Momentum for MomentumOptimizer or RMSPropOptimizer\n",
    "# DNN model hyperparameters\n",
    "DNN_OPTIMIZER = 'Adagrad'\n",
    "DNN_OPTIMIZER_LR = 0.1\n",
    "DNN_L1_REG = 0.0           # L1 Regularization rate for FtrlOptimizer\n",
    "DNN_MOMENTUM = 0.9         # Momentum for MomentumOptimizer or RMSPropOptimizer\n",
    "# Layer dimensions are defined separately to make this work with AzureML Hyperdrive\n",
    "DNN_HIDDEN_LAYER_1 = 0     # Set 0 to not use this layer\n",
    "DNN_HIDDEN_LAYER_2 = 128   # Set 0 to not use this layer\n",
    "DNN_HIDDEN_LAYER_3 = 256   # Set 0 to not use this layer\n",
    "DNN_HIDDEN_LAYER_4 = 32    # With this setting, DNN hidden units will be = [512, 256, 128, 128]\n",
    "DNN_USER_DIM = 4\n",
    "DNN_ITEM_DIM = 4\n",
    "DNN_DROPOUT = 0.4\n",
    "DNN_BATCH_NORM = 1         # 1 to use batch normalization, 0 if not.\n",
    "\n",
    "# Set cache directory path if want to keep the model checkpoints\n",
    "MODEL_DIR = './cahce/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_data \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserId  MovieId  Rating   timestamp         MovieName  \\\n0       1        1     4.0   964982703  Toy Story (1995)   \n1       5        1     4.0   847434962  Toy Story (1995)   \n2       7        1     4.5  1106635946  Toy Story (1995)   \n3      15        1     2.5  1510577970  Toy Story (1995)   \n4      17        1     4.5  1305696483  Toy Story (1995)   \n\n                                 Genres_string  \n0  Adventure|Animation|Children|Comedy|Fantasy  \n1  Adventure|Animation|Children|Comedy|Fantasy  \n2  Adventure|Animation|Children|Comedy|Fantasy  \n3  Adventure|Animation|Children|Comedy|Fantasy  \n4  Adventure|Animation|Children|Comedy|Fantasy  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(no genres listed)' 'Action' 'Adventure' 'Animation' 'Children' 'Comedy'\n 'Crime' 'Documentary' 'Drama' 'Fantasy' 'Film-Noir' 'Horror' 'IMAX'\n 'Musical' 'Mystery' 'Romance' 'Sci-Fi' 'Thriller' 'War' 'Western']"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_data \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     MovieId                                Genres_string  \\\n0          1  Adventure|Animation|Children|Comedy|Fantasy   \n215        3                               Comedy|Romance   \n267        6                        Action|Crime|Thriller   \n369       47                             Mystery|Thriller   \n572       50                       Crime|Mystery|Thriller   \n\n                                                Genres  \n0    [0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n215  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n267  [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n369  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n572  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, ...  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# 데이터 전처리\n",
    "# 1. Rating Data & Genres Data\n",
    "###############################\n",
    "df_rating = pd.read_csv('./data/100K_Latest/ratings.csv', \n",
    "                        sep=\",\", skiprows=1, header=None, \n",
    "                        names=[USER_COL, ITEM_COL, RATING_COL, 'timestamp'], engine='python')\n",
    "df_movie = pd.read_csv('./data/100K_Latest/movies.csv', \n",
    "                       sep=\",\", skiprows=1, header=None, \n",
    "                       names=[ITEM_COL, 'MovieName', 'Genres_string'], engine='python')\n",
    "\n",
    "# print('df_ratings \\n', df_rating.head())\n",
    "# print('df_movie \\n', df_movie.head())\n",
    "\n",
    "df_data = pd.merge(df_rating, df_movie)\n",
    "\n",
    "print('df_data \\n', df_data.head())\n",
    "\n",
    "\n",
    "###############################\n",
    "# 데이터 전처리\n",
    "# 2. Feature 인코딩\n",
    "###############################\n",
    "# Encode 'genres' into int array (multi-hot representation) to use as item features\n",
    "genres_encoder = sklearn.preprocessing.MultiLabelBinarizer()\n",
    "df_data[ITEM_FEAT_COL] = genres_encoder.fit_transform(\n",
    "    df_data['Genres_string'].apply(lambda s: s.split(\"|\"))\n",
    ").tolist()\n",
    "print(\"Genres:\", genres_encoder.classes_)\n",
    "\n",
    "print(\"df_data \\n\", df_data.drop_duplicates(ITEM_COL)[[ITEM_COL, 'Genres_string', ITEM_FEAT_COL]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train = 75627, test = 25209"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# Train, Test 데이터 나누기\n",
    "###############################\n",
    "train, test = python_random_split(\n",
    "    df_data.drop('Genres_string', axis=1),  # We don't need Genres original string column\n",
    "    ratio=0.75,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Train = {}, test = {}\".format(len(train), len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num items = 9724, num users = 610"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# item, user 수 확인\n",
    "###############################\n",
    "# Unique items in the dataset\n",
    "if ITEM_FEAT_COL is None:\n",
    "    items = df_data.drop_duplicates(ITEM_COL)[[ITEM_COL]].reset_index(drop=True)\n",
    "    item_feat_shape = None\n",
    "else:\n",
    "    items = df_data.drop_duplicates(ITEM_COL)[[ITEM_COL, ITEM_FEAT_COL]].reset_index(drop=True)\n",
    "    item_feat_shape = len(items[ITEM_FEAT_COL][0])\n",
    "# Unique users in the dataset\n",
    "users = df_data.drop_duplicates(USER_COL)[[USER_COL]].reset_index(drop=True)\n",
    "\n",
    "print(\"Num items = {}, num users = {}\".format(len(items), len(users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN hidden units ="
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 256, 32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 610 users to 4-dim vector"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 9724 items to 4-dim vector"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# 최소 한 번은 학습 & 체크포인트 저장하도록 세팅\n",
    "##############################################\n",
    "train_steps = max(1, EPOCHS * len(train) // BATCH_SIZE)\n",
    "save_checkpoints_steps = max(1, train_steps // 5)\n",
    "\n",
    "##########################################################\n",
    "# MODEL_DIR 에 모델이 존재하면 존재하는 모델을 학습을 이어서\n",
    "# 모델의 구조가 다르면 에러 발생\n",
    "##########################################################\n",
    "if MODEL_DIR is None:\n",
    "    tmp_dir = TemporaryDirectory()\n",
    "    MODEL_DIR = tmp_dir.name\n",
    "\n",
    "\n",
    "############\n",
    "# 모델 세팅\n",
    "############\n",
    "DNN_HIDDEN_UNITS = [DNN_HIDDEN_LAYER_1, DNN_HIDDEN_LAYER_2, DNN_HIDDEN_LAYER_3, DNN_HIDDEN_LAYER_4]\n",
    "DNN_HIDDEN_UNITS = [h for h in DNN_HIDDEN_UNITS if h > 0] \n",
    "if MODEL_TYPE is 'deep' or MODEL_TYPE is 'wide_deep':\n",
    "    print(\"DNN hidden units =\", DNN_HIDDEN_UNITS)\n",
    "    print(\"Embedding {} users to {}-dim vector\".format(len(users), DNN_USER_DIM))\n",
    "    print(\"Embedding {} items to {}-dim vector\".format(len(items), DNN_ITEM_DIM))\n",
    "\n",
    "##########################\n",
    "# 옵티마이저 파라미터 세팅\n",
    "##########################\n",
    "linear_params = {}\n",
    "if LINEAR_OPTIMIZER == 'Ftrl':\n",
    "    linear_params['l1_regularization_strength'] = LINEAR_L1_REG\n",
    "elif LINEAR_OPTIMIZER == 'Momentum' or LINEAR_OPTIMIZER == 'RMSProp':\n",
    "    linear_params['momentum'] = LINEAR_MOMENTUM\n",
    "\n",
    "dnn_params = {}\n",
    "if DNN_OPTIMIZER == 'Ftrl':\n",
    "    dnn_params['l1_regularization_strength'] = DNN_L1_REG\n",
    "elif DNN_OPTIMIZER == 'Momentum' or DNN_OPTIMIZER == 'RMSProp':\n",
    "    dnn_params['momentum'] = DNN_MOMENTUM\n",
    "\n",
    "print(\"\\n\", linear_params, dnn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nFeature specs:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossedColumn(keys=(VocabularyListCategoricalColumn(key='UserId', vocabulary_list=(1, 5, 7, 15, 17, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='UserId', vocabulary_list=(1,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='MovieId', vocabulary_list=(1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumericColumn(key='Genres', shape=(20,), default_value=None, dtype=tf.float32, normalizer_fn=None)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Model Feature 세팅 - wide(linear) & deep(dnn)\n",
    "################################################\n",
    "wide_columns, deep_columns = wide_deep.build_feature_columns(\n",
    "    users=users[USER_COL].values,\n",
    "    items=items[ITEM_COL].values,\n",
    "    user_col=USER_COL,\n",
    "    item_col=ITEM_COL,\n",
    "    item_feat_col=ITEM_FEAT_COL,\n",
    "    user_dim=DNN_USER_DIM,\n",
    "    item_dim=DNN_ITEM_DIM,\n",
    "    item_feat_shape=item_feat_shape,\n",
    "    model_type=MODEL_TYPE,\n",
    ")\n",
    "\n",
    "print(\"\\nFeature specs:\")\n",
    "for c in wide_columns + deep_columns:\n",
    "    print(str(c)[:100], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:32:15.039165 11028 deprecation_wrapper.py:119] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\tf_utils.py:82: The name tf.train.FtrlOptimizer is deprecated. Please use tf.compat.v1.train.FtrlOptimizer instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:32:15.052129 11028 deprecation_wrapper.py:119] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\tf_utils.py:78: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# 세팅한 파라미터에 기반한 모델 빌드\n",
    "####################################\n",
    "model = wide_deep.build_model(\n",
    "    model_dir=MODEL_DIR,\n",
    "    wide_columns=wide_columns,\n",
    "    deep_columns=deep_columns,\n",
    "    linear_optimizer=tf_utils.build_optimizer(LINEAR_OPTIMIZER, LINEAR_OPTIMIZER_LR, **linear_params),\n",
    "    dnn_optimizer=tf_utils.build_optimizer(DNN_OPTIMIZER, DNN_OPTIMIZER_LR, **dnn_params),\n",
    "    dnn_hidden_units=DNN_HIDDEN_UNITS,\n",
    "    dnn_dropout=DNN_DROPOUT,\n",
    "    dnn_batch_norm=(DNN_BATCH_NORM==1),\n",
    "    log_every_n_iter=max(1, train_steps//20),  # log 20 times\n",
    "    save_checkpoints_steps=save_checkpoints_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = {\n",
    "    'col_user': USER_COL,\n",
    "    'col_item': ITEM_COL,\n",
    "    'col_rating': RATING_COL,\n",
    "    'col_prediction': 'prediction'\n",
    "}\n",
    "\n",
    "#####################################\n",
    "# user와 item의 전체 조합(cross join)\n",
    "#####################################\n",
    "ranking_pool = user_item_pairs(\n",
    "    user_df=users,\n",
    "    item_df=items,\n",
    "    user_col=USER_COL,\n",
    "    item_col=ITEM_COL,\n",
    "    user_item_filter_df=train,  # Remove seen items\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:43:49.105174 11028 deprecation_wrapper.py:119] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\tf_utils.py:52: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:43:49.108168 11028 deprecation_wrapper.py:119] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\tf_utils.py:52: The name tf.estimator.inputs.numpy_input_fn is deprecated. Please use tf.compat.v1.estimator.inputs.numpy_input_fn instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define training hooks to track performance while training\n",
    "hooks = []\n",
    "if EVALUATE_WHILE_TRAINING:\n",
    "    evaluation_logger = tf_utils.MetricsLogger()\n",
    "    metrics = (m for m in (RANKING_METRICS, RATING_METRICS) if len(m) > 0)\n",
    "    for ms in metrics:\n",
    "        hooks.append(\n",
    "            tf_utils.evaluation_log_hook(\n",
    "                model,\n",
    "                logger=evaluation_logger,\n",
    "                true_df=test,\n",
    "                y_col=RATING_COL,\n",
    "                eval_df=ranking_pool if ms==RANKING_METRICS else test.drop(RATING_COL, axis=1),\n",
    "                every_n_iter=save_checkpoints_steps,\n",
    "                model_dir=MODEL_DIR,\n",
    "                eval_fns=[getattr(python_evaluation, m) for m in ms],\n",
    "                **({**cols, 'k': TOP_K} if ms==RANKING_METRICS else cols)\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Define training input (sample feeding) function\n",
    "train_fn = tf_utils.pandas_input_fn(\n",
    "    df=train,\n",
    "    y_col=RATING_COL,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=None,  # None == run forever. We use steps=TRAIN_STEPS instead.\n",
    "    shuffle=True,\n",
    "    num_threads=num_cpus-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps = 59083, Batch size = 64 (num epochs = 50)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:53:36.589709 11028 deprecation.py:323] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\venv\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:53:36.783190 11028 deprecation.py:323] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\venv\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nTo construct input pipelines, use the `tf.data` module."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:53:36.791169 11028 deprecation.py:323] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\venv\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nTo construct input pipelines, use the `tf.data` module."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 19:53:36.873950 11028 estimator.py:1145] Calling model_fn."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:53:36.904866 11028 deprecation.py:506] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:53:37.531191 11028 deprecation.py:323] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\venv\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:3038: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:53:38.758909 11028 deprecation.py:323] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\venv\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:2655: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:53:41.139543 11028 deprecation.py:323] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\venv\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\linear.py:308: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.cast` instead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:53:42.330362 11028 deprecation.py:506] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\venv\\lib\\site-packages\\tensorflow\\python\\training\\adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 19:53:42.682455 11028 estimator.py:1147] Done calling model_fn."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 19:53:42.685410 11028 basic_session_run_hooks.py:541] Create CheckpointSaverHook."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:53:43.060409 11028 deprecation_wrapper.py:119] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\tf_utils.py:199: The name tf.summary.FileWriterCache is deprecated. Please use tf.compat.v1.summary.FileWriterCache instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:53:43.063401 11028 deprecation_wrapper.py:119] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\tf_utils.py:200: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 19:53:43.527197 11028 monitored_session.py:240] Graph was finalized."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 19:53:45.195738 11028 session_manager.py:500] Running local_init_op."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 19:53:45.371228 11028 session_manager.py:502] Done running local_init_op."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:53:45.441043 11028 deprecation.py:323] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\venv\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nTo construct input pipelines, use the `tf.data` module."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 19:53:46.668798 11028 basic_session_run_hooks.py:606] Saving checkpoints for 0 into ./cahce/model\\model.ckpt."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:53:47.128779 11028 deprecation_wrapper.py:119] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\tf_utils.py:207: The name tf.train.SessionRunArgs is deprecated. Please use tf.estimator.SessionRunArgs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 19:53:49.433370 11028 basic_session_run_hooks.py:262] loss = 1053.0758, step = 1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:53:53.524430 11028 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 915 vs previous value: 915. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 19:54:02.302960 11028 basic_session_run_hooks.py:692] global_step/sec: 231.634"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 19:54:02.309948 11028 basic_session_run_hooks.py:260] loss = 67.35977, step = 2955 (12.882 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:54:02.453554 11028 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 2967 vs previous value: 2967. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 19:54:18.555501 11028 basic_session_run_hooks.py:692] global_step/sec: 180.285"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 19:54:18.562483 11028 basic_session_run_hooks.py:260] loss = 42.57393, step = 5909 (16.253 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:54:25.107981 11028 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 6409 vs previous value: 6409. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:54:31.978610 11028 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 6816 vs previous value: 6816. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 19:54:51.608684 11028 basic_session_run_hooks.py:692] global_step/sec: 89.3711"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 19:54:51.612673 11028 basic_session_run_hooks.py:260] loss = 47.872826, step = 8863 (33.050 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:54:53.151566 11028 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 9008 vs previous value: 9008. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 19:55:13.660748 11028 basic_session_run_hooks.py:606] Saving checkpoints for 11816 into ./cahce/model\\model.ckpt."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:55:14.707947 11028 deprecation_wrapper.py:119] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\tf_utils.py:218: The name tf.logging.get_verbosity is deprecated. Please use tf.compat.v1.logging.get_verbosity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:55:14.711937 11028 deprecation_wrapper.py:119] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\tf_utils.py:219: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 19:55:14.717920 11028 deprecation_wrapper.py:119] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\tf_utils.py:219: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:00:20.586909 11028 basic_session_run_hooks.py:692] global_step/sec: 8.97934"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:00:20.655721 11028 basic_session_run_hooks.py:260] loss = 56.186836, step = 11817 (329.043 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:00:34.333011 11028 basic_session_run_hooks.py:692] global_step/sec: 214.882"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:00:34.336995 11028 basic_session_run_hooks.py:260] loss = 42.11669, step = 14771 (13.681 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:00:51.023379 11028 basic_session_run_hooks.py:692] global_step/sec: 176.988"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:00:51.029362 11028 basic_session_run_hooks.py:260] loss = 28.052593, step = 17725 (16.691 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:01:05.064837 11028 basic_session_run_hooks.py:692] global_step/sec: 210.377"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:01:05.068825 11028 basic_session_run_hooks.py:260] loss = 48.668915, step = 20679 (14.040 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:01:18.537811 11028 basic_session_run_hooks.py:606] Saving checkpoints for 23632 into ./cahce/model\\model.ckpt."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:06:11.505718 11028 basic_session_run_hooks.py:692] global_step/sec: 9.63971"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:06:11.643172 11028 basic_session_run_hooks.py:260] loss = 38.190994, step = 23633 (306.569 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:06:23.621141 11028 basic_session_run_hooks.py:692] global_step/sec: 243.841"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:06:23.624135 11028 basic_session_run_hooks.py:260] loss = 43.85072, step = 26587 (11.987 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:06:37.942849 11028 basic_session_run_hooks.py:692] global_step/sec: 206.246"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:06:37.945841 11028 basic_session_run_hooks.py:260] loss = 52.082386, step = 29541 (14.322 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:06:56.915120 11028 basic_session_run_hooks.py:692] global_step/sec: 155.701"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:06:56.918113 11028 basic_session_run_hooks.py:260] loss = 31.841549, step = 32495 (18.972 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:07:10.772070 11028 basic_session_run_hooks.py:606] Saving checkpoints for 35448 into ./cahce/model\\model.ckpt."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:13:03.702912 11028 basic_session_run_hooks.py:692] global_step/sec: 8.05373"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:13:03.751780 11028 basic_session_run_hooks.py:260] loss = 49.31167, step = 35449 (366.834 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:13:17.851082 11028 basic_session_run_hooks.py:692] global_step/sec: 208.79"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:13:17.855072 11028 basic_session_run_hooks.py:260] loss = 29.02741, step = 38403 (14.103 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:13:34.842650 11028 basic_session_run_hooks.py:692] global_step/sec: 173.841"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:13:34.845641 11028 basic_session_run_hooks.py:260] loss = 48.463264, step = 41357 (16.991 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:13:46.663045 11028 basic_session_run_hooks.py:692] global_step/sec: 249.907"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:13:46.667034 11028 basic_session_run_hooks.py:260] loss = 32.53171, step = 44311 (11.820 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:13:58.764690 11028 basic_session_run_hooks.py:606] Saving checkpoints for 47264 into ./cahce/model\\model.ckpt."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:17:16.856030 11028 basic_session_run_hooks.py:692] global_step/sec: 14.0538"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:17:16.927842 11028 basic_session_run_hooks.py:260] loss = 31.52892, step = 47265 (210.256 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:17:29.300757 11028 basic_session_run_hooks.py:692] global_step/sec: 237.389"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:17:29.302752 11028 basic_session_run_hooks.py:260] loss = 42.666824, step = 50219 (12.381 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:17:41.593886 11028 basic_session_run_hooks.py:692] global_step/sec: 240.277"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:17:41.598873 11028 basic_session_run_hooks.py:260] loss = 42.538452, step = 53173 (12.296 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:17:53.435226 11028 basic_session_run_hooks.py:692] global_step/sec: 249.486"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:17:53.444206 11028 basic_session_run_hooks.py:260] loss = 36.007195, step = 56127 (11.845 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:18:06.565119 11028 basic_session_run_hooks.py:606] Saving checkpoints for 59080 into ./cahce/model\\model.ckpt."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 20:18:06.781541 11028 deprecation.py:323] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\venv\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to delete files with this prefix."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:21:20.343625 11028 basic_session_run_hooks.py:692] global_step/sec: 14.2768"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:21:20.459316 11028 basic_session_run_hooks.py:260] loss = 34.39142, step = 59081 (207.009 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:21:20.478265 11028 basic_session_run_hooks.py:606] Saving checkpoints for 59083 into ./cahce/model\\model.ckpt."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:21:23.102496 11028 estimator.py:368] Loss for final step: 33.314835."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training steps = {}, Batch size = {} (num epochs = {})\".format(train_steps, BATCH_SIZE, EPOCHS))\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "#################\n",
    "# 모델 학습 시작\n",
    "#################\n",
    "try:\n",
    "    model.train(\n",
    "        input_fn=train_fn,\n",
    "        hooks=hooks,\n",
    "        steps=train_steps\n",
    "    )\n",
    "except tf.train.NanLossDuringTrainingError:\n",
    "    raise ValueError(\n",
    "        \"\"\"Training stopped with NanLossDuringTrainingError.\n",
    "        Try other optimizers, smaller batch size and/or smaller learning rate.\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'papermill' has no attribute 'record'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-24d23dbba14c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mEVALUATE_WHILE_TRAINING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevaluation_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eval_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'papermill' has no attribute 'record'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "if EVALUATE_WHILE_TRAINING:\n",
    "    for m, v in evaluation_logger.get_log().items():\n",
    "        pm.record(\"eval_{}\".format(m), v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:26:32.792201 11028 estimator.py:1145] Calling model_fn."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:26:34.226369 11028 estimator.py:1147] Done calling model_fn."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:26:34.580422 11028 monitored_session.py:240] Graph was finalized."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:26:34.590394 11028 saver.py:1280] Restoring parameters from ./cahce/model\\model.ckpt-59083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:26:34.756949 11028 session_manager.py:500] Running local_init_op."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:26:34.826763 11028 session_manager.py:502] Done running local_init_op."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_df ::: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       UserId  MovieId   timestamp  \\\n67037     551    34162  1504925858   \n42175     232    59421  1217541086   \n93850     288     8880  1095780696   \n6187      414     1080   961595418   \n12229     577     2406   945965771   \n7433      502     1206  1111757634   \n53802     137     6787  1204859228   \n65098      97     4025  1047481289   \n68041     490    88125  1324376714   \n11854     593     2291  1181008449   \n\n                                               MovieName  \\\n67037                            Wedding Crashers (2005)   \n42175                    What Happens in Vegas... (2008)   \n93850                                        Mask (1985)   \n6187                 Monty Python's Life of Brian (1979)   \n12229                         Romancing the Stone (1984)   \n7433                          Clockwork Orange, A (1971)   \n53802                     All the President's Men (1976)   \n65098                           Miss Congeniality (2000)   \n68041  Harry Potter and the Deathly Hallows: Part 2 (...   \n11854                         Edward Scissorhands (1990)   \n\n                                                  Genres  prediction  \n67037  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    3.717763  \n42175  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    3.195282  \n93850  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...    3.250296  \n6187   [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    3.895330  \n12229  [0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    3.740880  \n7433   [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...    3.921042  \n53802  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...    4.142208  \n65098  [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...    4.038630  \n68041  [0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, ...    3.189037  \n11854  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...    3.596118  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 0.8912505265425538, 'mae': 0.6752194901811193, 'rsquared': 0.26311857056258414, 'exp_var': 0.2633145675924853}"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if len(RATING_METRICS) > 0:\n",
    "    predictions = list(model.predict(input_fn=tf_utils.pandas_input_fn(df=test)))\n",
    "    prediction_df = test.drop(RATING_COL, axis=1)\n",
    "    prediction_df['prediction'] = [p['predictions'][0] for p in predictions]    \n",
    "    \n",
    "    print(\"prediction_df ::: \\n\", prediction_df[:10])\n",
    "    \n",
    "    rating_results = {}\n",
    "    for m in RATING_METRICS:\n",
    "        fn = getattr(python_evaluation, m)\n",
    "        result = fn(test, prediction_df, **cols)\n",
    "        # pm.record(m, result)\n",
    "        rating_results[m] = result\n",
    "    print(rating_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:26:59.349225 11028 estimator.py:1145] Calling model_fn."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:27:00.054309 11028 estimator.py:1147] Done calling model_fn."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:27:00.285690 11028 monitored_session.py:240] Graph was finalized."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:27:00.292671 11028 saver.py:1280] Restoring parameters from ./cahce/model\\model.ckpt-59083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:27:00.390410 11028 session_manager.py:500] Running local_init_op."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 20:27:00.431303 11028 session_manager.py:502] Done running local_init_op."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_df ::: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserId  MovieId                                             Genres  \\\n0     289     3087  [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n1     233    54503  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2     289      121  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n3     476     6597  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4     352      223  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n5     245     7026  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n6     169      898  [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n7     387    79590  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n8     513     4928  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n9     450     6832  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n\n   prediction  \n0    2.782947  \n1    3.388410  \n2    3.898042  \n3    3.802190  \n4    4.146657  \n5    1.674149  \n6    4.720969  \n7    2.606355  \n8    4.018032  \n9    4.089769  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map_at_k': 1.0714668381013606e-06, 'ndcg_at_k': 0.0001804030871295739, 'precision_at_k': 0.0001639344262295082, 'recall_at_k': 3.2144005143040823e-06}"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if len(RANKING_METRICS) > 0:\n",
    "    predictions = list(model.predict(input_fn=tf_utils.pandas_input_fn(df=ranking_pool)))\n",
    "    prediction_df = ranking_pool.copy()\n",
    "    prediction_df['prediction'] = [p['predictions'][0] for p in predictions]\n",
    "\n",
    "    print(\"prediction_df ::: \\n\", prediction_df[:10])\n",
    "    \n",
    "    ranking_results = {}\n",
    "    for m in RANKING_METRICS:\n",
    "        fn = getattr(python_evaluation, m)\n",
    "        result = fn(test, prediction_df, **{**cols, 'k': TOP_K})\n",
    "        # pm.record(m, result)\n",
    "        ranking_results[m] = result\n",
    "    print(ranking_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(EXPORT_DIR_BASE, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to b'./outputs/model\\\\1563665171'\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "train_rcvr_fn = tf.contrib.estimator.build_supervised_input_receiver_fn_from_input_fn(\n",
    "    train_fn\n",
    ")\n",
    "eval_rcvr_fn = tf.contrib.estimator.build_supervised_input_receiver_fn_from_input_fn(\n",
    "    tf_utils.pandas_input_fn(df=test, y_col=RATING_COL)\n",
    ")\n",
    "serve_rcvr_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "    tf.feature_column.make_parse_example_spec(wide_columns+deep_columns)\n",
    ")\n",
    "rcvr_fn_map = {\n",
    "    tf.estimator.ModeKeys.TRAIN: train_rcvr_fn,\n",
    "    tf.estimator.ModeKeys.EVAL: eval_rcvr_fn,\n",
    "    tf.estimator.ModeKeys.PREDICT: serve_rcvr_fn\n",
    "}\n",
    "\n",
    "export_dir = tf.contrib.estimator.export_all_saved_models(\n",
    "    model,\n",
    "    export_dir_base=EXPORT_DIR_BASE,\n",
    "    input_receiver_fn_map=rcvr_fn_map\n",
    ")\n",
    "# pm.record('saved_model_dir', str(export_dir))\n",
    "print(\"Model exported to\", str(export_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        UserId  MovieId  Rating   timestamp                   MovieName  \\\n97717      414     4985     1.0  1008691553               Sheena (1984)   \n100124     599     6860     2.0  1519345916             Mobsters (1991)   \n25952      387      300     3.0  1154681852            Quiz Show (1994)   \n25871      414      266     5.0   961512595  Legends of the Fall (1994)   \n97255      477    59141     4.5  1241396097        Son of Rambow (2007)   \n\n                                                   Genres  \n97717   [0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n100124  [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n25952   [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n25871   [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n97255   [0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_genre \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     MovieId                                             Genres\n0          1  [0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...\n215        3  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n267        6  [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n369       47  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n572       50  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, ..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_my_data \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserId  MovieId  Rating    timestamp  \\\n0     999        1     4.0  964982703.0   \n1     999        2     4.0  964983148.0   \n2     999       36     4.0  964997388.0   \n3     999       48     3.0  965002283.0   \n4     999       50     5.0  965003173.0   \n\n                                              Genres  \n0  [0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n1  [0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n2  [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n3  [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...  \n4  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, ...  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:45:11.124503 11028 estimator.py:1145] Calling model_fn."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:45:14.733361 11028 estimator.py:1147] Done calling model_fn."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:45:14.737352 11028 basic_session_run_hooks.py:541] Create CheckpointSaverHook."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:45:15.467157 11028 monitored_session.py:240] Graph was finalized."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:45:15.589876 11028 saver.py:1280] Restoring parameters from ./cahce/model\\model.ckpt-59083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 21:45:16.600005 11028 deprecation.py:323] From D:\\01.Programming\\PycharmProjects\\Recommenders-movielens\\venv\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file utilities to get mtimes."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:45:16.768841 11028 session_manager.py:500] Running local_init_op."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:45:16.895826 11028 session_manager.py:502] Done running local_init_op."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:45:18.374121 11028 basic_session_run_hooks.py:606] Saving checkpoints for 59083 into ./cahce/model\\model.ckpt."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:45:20.534312 11028 basic_session_run_hooks.py:262] loss = 25.261837, step = 59084"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:45:32.929731 11028 basic_session_run_hooks.py:692] global_step/sec: 238.256"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:45:32.931727 11028 basic_session_run_hooks.py:260] loss = 1.9528362, step = 62038 (12.398 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:45:44.962759 11028 basic_session_run_hooks.py:692] global_step/sec: 245.491"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:45:44.964565 11028 basic_session_run_hooks.py:260] loss = 1.2825283, step = 64992 (12.033 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:45:56.787578 11028 basic_session_run_hooks.py:692] global_step/sec: 249.814"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:45:56.789935 11028 basic_session_run_hooks.py:260] loss = 0.60972863, step = 67946 (11.825 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:50:09.246234 11028 basic_session_run_hooks.py:606] Saving checkpoints for 70899 into ./cahce/model\\model.ckpt."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:50:10.190173 11028 basic_session_run_hooks.py:692] global_step/sec: 11.6573"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:50:10.194161 11028 basic_session_run_hooks.py:260] loss = 0.40095347, step = 70900 (253.404 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:50:29.790070 11028 basic_session_run_hooks.py:692] global_step/sec: 150.715"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:50:29.795057 11028 basic_session_run_hooks.py:260] loss = 0.76796615, step = 73854 (19.601 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:50:49.431553 11028 basic_session_run_hooks.py:692] global_step/sec: 150.404"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:50:49.435543 11028 basic_session_run_hooks.py:260] loss = 0.5174681, step = 76808 (19.640 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:51:08.969312 11028 basic_session_run_hooks.py:692] global_step/sec: 151.187"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 21:51:08.974328 11028 basic_session_run_hooks.py:260] loss = 0.5706986, step = 79762 (19.539 sec)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# 내가 매긴 평가를 바탕으로 추천\n",
    "# my_ratings.csv에 각 영화에 대한 평점 입력\n",
    "# UserId : 999\n",
    "###########################################\n",
    "# 데이터 불러오기\n",
    "df_my_rating = pd.read_csv('./data/100K_Latest/my_ratings.csv', \n",
    "                        sep=\",\", skiprows=1, header=None, \n",
    "                        names=[USER_COL, ITEM_COL, RATING_COL, 'timestamp', 'movieName'], engine='python')\n",
    "\n",
    "# movieNmae 컬럼 삭제\n",
    "del df_my_rating['movieName']\n",
    "# 평가하지 않은 영화 삭제\n",
    "df_my_rating = df_my_rating.dropna(axis=0)\n",
    "\n",
    "# print(\"df_my_rating ::: \\n\", df_my_rating)\n",
    "# print(\"test ::: \\n\", test)\n",
    "\n",
    "# df_my_rating = pd.merge(df_my_rating, df_movie)\n",
    "df_genre = df_data.drop_duplicates(ITEM_COL)[[ITEM_COL, ITEM_FEAT_COL]]\n",
    "df_my_rating = pd.merge(df_my_rating, df_genre, on=\"MovieId\")\n",
    "\n",
    "print('train \\n', train.head())\n",
    "print('df_genre \\n', df_genre.head())\n",
    "print('df_my_data \\n', df_my_rating.head())\n",
    "\n",
    "\n",
    "##########################\n",
    "# 모델 추가 학습 학습 시작\n",
    "##########################\n",
    "my_train_fn = tf_utils.pandas_input_fn(\n",
    "    df=df_my_rating,\n",
    "    y_col=RATING_COL,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=None,  # None == run forever. We use steps=TRAIN_STEPS instead.\n",
    "    shuffle=True,\n",
    "    num_threads=num_cpus-1\n",
    ")\n",
    "\n",
    "try:\n",
    "    model.train(\n",
    "        input_fn=my_train_fn,\n",
    "        hooks=hooks,\n",
    "        steps=train_steps\n",
    "    )\n",
    "except tf.train.NanLossDuringTrainingError:\n",
    "    raise ValueError(\n",
    "        \"\"\"Training stopped with NanLossDuringTrainingError.\n",
    "        Try other optimizers, smaller batch size and/or smaller learning rate.\"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
